"""
Command-line interface for the RAG system with memory management.
"""
import json
import gc
import torch
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any

from .rag_chat import RAGChatSystem


class CLIApplication:
    """Main CLI application for RAG Q&A with pre-processed data."""

    def __init__(self):
        self.rag_system = None
        self.current_results = None
        self.current_text = None
        self.current_metadata = None

    def get_file_path(self) -> Optional[Path]:
        """Prompt user for processed file path."""
        print("\n" + "=" * 70)
        print("ğŸ“ RAG Q&A SYSTEM - PRE-PROCESSED DATA")
        print("=" * 70)

        while True:
            file_path = input(
                "\nEnter the path to your processed JSON file (or 'quit' to exit): "
            ).strip()

            if file_path.lower() in ["quit", "exit", "q"]:
                return None

            # Handle quotes
            file_path = file_path.strip('"').strip("'")
            path = Path(file_path)

            if not path.exists():
                print(f"âŒ File not found: {file_path}")
                print("   Please check the path and try again.")
                continue

            if not path.is_file():
                print(f"âŒ Not a file: {file_path}")
                continue

            if path.suffix not in [".json"]:
                print(f"âš ï¸  Warning: Expected .json file, got {path.suffix}")
                confirm = input("Continue anyway? (y/n): ").lower()
                if confirm != "y":
                    continue

            return path

    def load_processed_data(self, file_path: Path) -> bool:
        """Load pre-processed data from JSON file."""
        try:
            print(f"\nğŸ“– Loading processed data: {file_path.name}")

            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            # Validate data structure
            if not self._validate_data_structure(data):
                print("âŒ Invalid data structure in JSON file")
                return False

            # Extract components
            self.current_metadata = data.get("metadata", {})

            # Reconstruct the results structure
            self.current_results = {
                "nodes": data.get("nodes", []),
                "edges": data.get("edges", []),
                "extracted_patterns": data.get("extracted_patterns", {}),
            }

            # Try to get original text if available
            self.current_text = data.get("original_text", "")

            # If no original text, create a summary from metadata
            if not self.current_text:
                self.current_text = f"Processed text data from {self.current_metadata.get('source_file', 'unknown source')}"

            print(f"âœ… Loaded data successfully:")
            print(f"   â€¢ Source: {self.current_metadata.get('source_file', 'Unknown')}")
            print(f"   â€¢ Processed: {self.current_metadata.get('processed_at', 'Unknown')}")
            print(f"   â€¢ Nodes: {len(self.current_results['nodes'])}")
            print(f"   â€¢ Edges: {len(self.current_results['edges'])}")

            return True

        except json.JSONDecodeError as e:
            print(f"âŒ Error parsing JSON file: {e}")
            return False
        except Exception as e:
            print(f"âŒ Error loading file: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _validate_data_structure(self, data: Dict[Any, Any]) -> bool:
        """Validate that the loaded data has the expected structure."""
        required_keys = ["nodes", "edges", "extracted_patterns"]

        for key in required_keys:
            if key not in data:
                print(f"âŒ Missing required key: {key}")
                return False

        # Check that nodes and edges are lists
        if not isinstance(data["nodes"], list):
            print("âŒ 'nodes' must be a list")
            return False

        if not isinstance(data["edges"], list):
            print("âŒ 'edges' must be a list")
            return False

        if not isinstance(data["extracted_patterns"], dict):
            print("âŒ 'extracted_patterns' must be a dictionary")
            return False

        return True

    def start_qa_session(self):
        """Start interactive Q&A session."""
        if not self.current_results or not self.current_text:
            print("âŒ No data loaded. Please load a processed file first.")
            return

        # Initialize RAG system
        if self.rag_system is None:
            print("\nğŸ¦™ Initializing Llama model for Q&A...")
            self.rag_system = RAGChatSystem()

        # Set context
        self.rag_system.set_context(
            self.current_results,
            self.current_text,
            self.current_results["extracted_patterns"],
        )

        print("\n" + "=" * 70)
        print("ğŸ’¬ RAG Q&A SESSION")
        print("=" * 70)
        print("Ask questions about the processed text.")
        print("Commands: 'quit' to exit | 'help' for examples | 'summary' for data overview")
        print("=" * 70 + "\n")

        while True:
            try:
                user_input = input("\nâ“ Your question: ").strip()

                if user_input.lower() in ["quit", "exit", "q"]:
                    print("\nğŸ‘‹ Ending Q&A session.")
                    break

                if user_input.lower() == "help":
                    self.show_help()
                    continue

                if user_input.lower() == "summary":
                    self.show_summary()
                    continue

                if user_input.lower() == "memory":
                    self._show_memory_usage()
                    continue

                if user_input.lower() == "info":
                    self.show_file_info()
                    continue

                if not user_input:
                    continue

                print("\nğŸ¤” Thinking...")
                answer = self.rag_system.generate_answer(user_input)
                print(f"\nğŸ’¡ Answer: {answer}")

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Q&A session interrupted.")
                break
            except Exception as e:
                print(f"\nâŒ Error: {str(e)}")

    def _show_memory_usage(self):
        """Show current memory usage."""
        try:
            import psutil
            memory = psutil.virtual_memory()
            print(f"\nğŸ’¾ Current Memory Usage:")
            print(f"   Used: {memory.used / (1024**3):.1f}GB")
            print(f"   Available: {memory.available / (1024**3):.1f}GB")
            print(f"   Total: {memory.total / (1024**3):.1f}GB")
            print(f"   Percentage: {memory.percent:.1f}%")

            if torch.cuda.is_available():
                print(f"\nğŸ”¥ GPU Memory:")
                print(f"   Allocated: {torch.cuda.memory_allocated() / (1024**3):.1f}GB")
                print(f"   Cached: {torch.cuda.memory_reserved() / (1024**3):.1f}GB")
        except ImportError:
            print("ğŸ’¾ Memory monitoring requires psutil: pip install psutil")

    def show_help(self):
        """Show example questions."""
        print("\nğŸ“š Example questions you can ask:")
        print("  â€¢ List all email addresses")
        print("  â€¢ What are all the phone numbers?")
        print("  â€¢ Show me all addresses")
        print("  â€¢ Who are the people mentioned in the text?")
        print("  â€¢ Tell me about [person name]")
        print("  â€¢ What is [person name]'s email address?")
        print("  â€¢ What is [person name]'s phone number?")
        print("  â€¢ Where does [person name] live?")
        print("  â€¢ What is the relationship between [person1] and [person2]?")
        print("  â€¢ How many people were identified?")
        print("\nğŸ”§ System commands:")
        print("  â€¢ 'memory' - Show current memory usage")
        print("  â€¢ 'summary' - Show data summary")
        print("  â€¢ 'info' - Show file information")
        print("  â€¢ 'help' - Show this help")
        print("  â€¢ 'quit' - Exit Q&A session")

    def show_summary(self):
        """Show data summary."""
        print("\nğŸ“Š Data Summary:")
        print(f"  â€¢ Total entities: {len(self.current_results['nodes'])}")
        print(f"  â€¢ Total relationships: {len(self.current_results['edges'])}")

        patterns = self.current_results["extracted_patterns"]
        print(f"  â€¢ Addresses found: {len(patterns.get('addresses', []))}")
        print(f"  â€¢ Phone numbers: {len(patterns.get('phones', []))}")
        print(f"  â€¢ Email addresses: {len(patterns.get('emails', []))}")
        print(f"  â€¢ Identities: {len(patterns.get('identities', []))}")

        # Show first few identities
        if patterns.get("identities"):
            print("\n  First few identities:")
            for identity in patterns["identities"][:3]:
                if isinstance(identity, dict):
                    print(f"    â€¢ {identity.get('person', 'Unknown')} (score: {identity.get('score', 0):.2f})")
                else:
                    print(f"    â€¢ {identity}")

    def show_file_info(self):
        """Show information about the loaded file."""
        print("\nğŸ“„ File Information:")
        if self.current_metadata:
            print(f"  â€¢ Source file: {self.current_metadata.get('source_file', 'Unknown')}")
            print(f"  â€¢ Processed at: {self.current_metadata.get('processed_at', 'Unknown')}")
            print(f"  â€¢ Original text length: {self.current_metadata.get('text_length', 'Unknown')} characters")
            print(f"  â€¢ Node count: {self.current_metadata.get('node_count', len(self.current_results['nodes']))}")
            print(f"  â€¢ Edge count: {self.current_metadata.get('edge_count', len(self.current_results['edges']))}")
        else:
            print("  â€¢ No metadata available")

    def cleanup_rag_system(self):
        """Clean up RAG system when done."""
        if self.rag_system is not None:
            print("\nğŸ§¹ Cleaning up Llama model...")

            # Clear model references
            if hasattr(self.rag_system, 'model'):
                del self.rag_system.model
            if hasattr(self.rag_system, 'tokenizer'):
                del self.rag_system.tokenizer

            del self.rag_system
            self.rag_system = None

            # Force cleanup
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            print("âœ… Llama model cleaned up")

    def run(self):
        """Main application loop with memory management."""
        print("\nğŸš€ Starting RAG Q&A System")
        print("   This tool will load pre-processed data and allow you to")
        print("   ask questions about the content using RAG (Retrieval-Augmented Generation).")
        print("   Memory is automatically managed.\n")

        try:
            while True:
                # Get file path
                file_path = self.get_file_path()
                if file_path is None:
                    print("\nğŸ‘‹ Goodbye!")
                    break

                # Load processed data
                if self.load_processed_data(file_path):
                    # Start Q&A session
                    self.start_qa_session()

                    # Clean up RAG system after Q&A session
                    self.cleanup_rag_system()

                    # Ask if user wants to load another file
                    print("\n" + "-" * 50)
                    another = input("Load another file? (y/n): ").lower()
                    if another != "y":
                        print("\nğŸ‘‹ Goodbye!")
                        break
                else:
                    # Ask if user wants to try again
                    retry = input("\nTry another file? (y/n): ").lower()
                    if retry != "y":
                        print("\nğŸ‘‹ Goodbye!")
                        break

        finally:
            # Final cleanup
            print("\nğŸ§¹ Final cleanup...")
            self.cleanup_rag_system()

            # Final garbage collection
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            print("âœ… All cleanup complete")

    def __del__(self):
        """Destructor to ensure cleanup."""
        try:
            self.cleanup_rag_system()
        except:
            pass  # Ignore errors during cleanup
